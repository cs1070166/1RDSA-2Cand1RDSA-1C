\section{CONVERGENCE ANALYSIS}
\label{sec:convergenceresults}
We make the same assumptions as those used in the analysis of \cite{spall}, with a 
few minor alterations. The assumptions are listed below.

\begin{enumerate}[label= \textbf{A\arabic*}.]
 \item The map $J:\mathbb{R}^p \rightarrow \mathbb{R}$ is Lipschitz continuous and 
 is differentiable with bounded second order derivatives. Further, 
 the map $L:\mathbb{R}^p \rightarrow \mathbb{R}$ defined as 
 $L(\theta)=-\nabla J(\theta)$ is Lipschitz continuous.
 \item The step-size sequences $a_n, \delta_n >0, \forall n $  satisfy
 \begin{equation*}\label{stepsizes}
 a_n,\delta_n \text{ as } n \rightarrow \infty, \sum_na_n=\infty,
 \sum_n \Big{(}\frac{a_n}{\delta_n}\Big{)}^2<\infty.
 \end{equation*}
 Further, $\frac{a_j}{a_n}\rightarrow 1$ as $n\rightarrow \infty$, for all
 $j \in \{n,n+1,\cdots,n+M\}$ for any given $M>0$ and $b_n=\frac{a_n}{\delta_n}$ is 
 such that $\frac{b_j}{b_n}\rightarrow 1$ as $n\rightarrow \infty$, for all
 $j \in \{n,n+1,n+2,\cdots,n+M\}.$

 \item The random vectors $d_n$, $n\geq0$ are mutually independent and identitcally
 distributed. Moreover $\mathbb{E}[d_nd_n^T]=I.$
 
 \item The iterates $\theta_n$ remain uniformly bounded almost surely, i.e,
 $$ \sup_n\|\theta_n\|<\infty, a.s.$$

 \item The set $G$ containing the globally asymptotically stable equilibria of the 
 ODE $\dot{\theta}(t)=-\nabla J(\theta(t)) $ (i.e., the local minima of $J$)
 is a compact subset of $\mathbb{R}^N.$
 
 \item The sequences $(M_{n}^{+},\F_n),(M_{n}^{-},\F_n), n\geq0 $ form martingale difference sequences.
 Further, $(M_{n}^{+},M_{n}^{-},n\geq0)$ are square integrable random variables satisfying
 $$\E[\|M_{n+1}^{\pm}\|^2|f_n]\leq K(1+\|\theta_n\|^2) \text{ a.s., } \forall n\geq0,$$
 for a given constant $K \geq 0$
 
\end{enumerate}
The following lemma is useful in obtaining the negative square root of $H$ i.e., 
$H^{-1/2}$. Also note that it takes only $O(p)$ operations to compute $H^{-1/2}$
using the lemma and circulant structure of $H^{-1/2}$.
\begin{lemma}
 \label{lemma: gen Sherman-Morisson}
Let $I$ be a $p \times p$ identity matrix and $u = 
\left[
 \begin{array}{cccc}
 1 \\ 1 \\  \vdots \\ 1 
\end{array} \right]$ be a $p \times 1$ all ones vector then
$$ (I+uu^T)^{-1/2}= I-\frac{uu^T}{p}+\frac{uu^T}{p\sqrt{(1+p)}}.$$
\end{lemma}

\begin{proof}
It is enough to show that
$$(I+uu^T)\Bigg{[}I-\frac{uu^T}{p}+\frac{uu^T}{p\sqrt{(1+p)}}\Bigg{]}^2=I.$$
Using $\|u\|^2=u^Tu=p$ we have
\begin{align*}
 & (I+uu^T)\Bigg{[}I-\frac{uu^T}{p}+\frac{uu^T}{p\sqrt{(1+p)}}\Bigg{]}^2\\
 &= (I+uu^T)\Bigg{[}I-\frac{uu^T}{p}+\frac{uu^T}{p\sqrt{(1+p)}}\\
 &-\frac{uu^T}{p}+\frac{uu^T}{p}-\frac{uu^T}{p\sqrt{(1+p)}}\\
 &+\frac{uu^T}{p\sqrt{(1+p)}}-\frac{uu^T}{p\sqrt{(1+p)}}+\frac{uu^T}{p(1+p)}\Bigg{]}\\
 &=(I+uu^T)\Bigg{[}I-uu^T(\frac{1}{p}-\frac{1}{p(1+p)})\Bigg{]}\\
 &=(I+uu^T)\Bigg{[}I-\frac{uu^T}{1+p}\Bigg{]}\\
 &=I+uu^T-\frac{uu^T}{1+p}-\frac{puu^T}{1+p}\\
 &=I
\end{align*}
proving the lemma.
\end{proof}
As defined earlier let \begin{equation*} H = \left[\begin{array}{cccc}
2 \ 1 \ 1 \cdots 1\\ 
1 \ 2 \ 1 \cdots 1 \\
\vdots \ \vdots \ \vdots \ \vdots\\
1 \ 1 \ 1 \cdots 2
\end{array}\right]
\end{equation*}
and $Q=\sqrt{p+1}[H^{-1/2},-H^{-1/2}u].$
Let the pertubations $d_n$ be the columns of Q. 
\begin{lemma}
 The perturbations $d_n$ choosen as columns of Q satisfy properties P1 and P2.
\end{lemma}
\begin{proof}
 Let $P=p+1$. Observe that as $n$ goes through one cycle from $1$ to $p+1$ we have
 $\sum\limits_{n=1}^{p+1}d_nd_n^T=QQ^T$ and 
 $\sum\limits_{n=1}^{p+1}d_n= Q\left[\begin{array}{cccc}
 u\\ 1 \end{array}\right]$.
 Now it is enough to show that
 $\sum\limits_{n=s+1}^{s+P}D_n=0$ and 
 $\sum\limits_{n=s+1}^{s+P}d_n= 0$ i.e., 
 $Q\left[\begin{array}{cccc}
 u\\ 1 \end{array}\right]=0 \text{ for the choice } s=0.$
 Consider
 \begin{align*}
 & \sum_{n=0}^{P}D_n\\
 & =\sum_{n=1}^{P}(d_nd_n^T-I)\\
 & =\sum_{n=1}^{P}(d_nd_n^T)-(p+1)I\\
 & =QQ^T-(p+1)I\\
 & =(p+1)([H^{-1/2},-H^{-1/2}u][H^{-1/2},-H^{-1/2}u]^T-I)\\
 & =(p+1)([H^{-1}+H^{-1/2}uu^TH^{-1/2}]-I)\\
 & =(p+1)(H^{-1/2}(I+uu^T)H^{-1/2}-I)\\
 & =(p+1)(H^{-1/2}(H)H^{-1/2}-I)\\
 & = 0.
 \end{align*}
 In addition,
 \begin{align*}
 Q\left[\begin{array}{cccc}
 u\\ 1 \end{array}\right]
 & = \sqrt{p+1}([H^{-1/2},-H^{-1/2}u]) \left[\begin{array}{cccc}
 u\\ 1 \end{array}\right]\\
 & = \sqrt{p+1}(H^{-1/2}u-H^{-1/2}u)\\
 & = 0
 \end{align*}
proving the lemma.
\end{proof}
\begin{lemma}
 Given any fixed integer $P>0$, $\|\theta_{m+k}-\theta_{m}\| \rightarrow 0$ $w.p.1,$ as
 $m \rightarrow \infty,$ for all $k \in {1,\cdots, P}$
\end{lemma}
\begin{proof}
 Fix a $k \in \{1,\cdots,P \}.$ Now
 \begin{align*}
 \begin{split}
 \theta_{n+k} = \theta_n & -\sum_{j=n}^{n+k-1}a_j\Bigg{(}\frac{J(\theta_j+\delta_j d_j)-J(\theta_j-\delta_j d_j)}{2\delta_j}\Bigg{)}d_j \\ 
  &-\sum_{j=n}^{n+k-1}a_jM_{j+1}
 \end{split}
 \end{align*}
 Thus,
 \begin{align*}
 \begin{split}
 \|\theta_{n+k}-\theta_n\| &\leq \sum_{j=n}^{n+k-1}a_j\Bigg{|}\Bigg{(}\frac{J(\theta_j+\delta_{j} d_j)-J(\theta_j-\delta_{j} d_j)}{2\delta_j}\Bigg{)}\Bigg{|}d_j\\
 &-\sum_{j=n}^{n+k-1}a_jM_{j+1}
\end{split}
 \end{align*}
Now clearly,
$$N_n=\sum_{j=0}^{n-1}a_jM_{j+1}, n\geq1,$$
forms a martingale sequence.
Further, from the assumption A6 we have,
\begin{align*}
& \sum_{m=0}^{n}\mathbb{E}[\|N_{m+1}-N_{m}\|^2|\mathcal{F}_{m}] \\
& =\sum_{m=0}^{n}\mathbb{E}[a_{m}^2\|M_{m+1}\|^2|\mathcal{F}_{m}]\\
& \leq \sum_{m=0}^{n}a_{m}^2K(1+\|\theta_n\|^2)
\end{align*}
From the assumption A4 the quadratic variation process of $N_n,n\geq0$ converges 
almost surely. Hence by the martingale convergence theorem, it follows that 
$N_n, n\geq0$ converges almost surely. Hence
$\|\sum\limits_{j=n}^{n+k-1}a_jM_{j+1}\|\rightarrow 0$ almost surely as $n\rightarrow \infty.$
Moreover
\begin{align*}
&\Big{\|}\Big{(}J(\theta_j+\delta_j d_j)-J(\theta_j-\delta_j d_j)\Big{)}d_j\Big{\|} \\
& \leq \Big{|}\Big{(}J(\theta_j+\delta_j d_j)-J(\theta_j-\delta_j d_j)\Big{)}\Big{|}\|d_j\|\\
& \leq C \Big{(}|J(\theta_j+\delta_j d_j)|+|J(\theta_j-\delta_j d_j)|\Big{)},
\end{align*}
since $\|d_j\|\leq C, \forall j \geq0.$
Note that
\begin{align*}
|J(\theta_j+\delta_j d_j)|-|J(0)| & \leq|J(\theta_j+\delta_j d_j)-J(0)| \\
& \leq \hat{B} \|\theta_j+\delta_j d_j\|,
\end{align*}
where $\hat{B}$ is the Lipschitz constant of the function $J(.).$ Hence,
$$|J(\theta_j+\delta_j d_j)|\leq \tilde{B}(1+\|\theta_j+\delta_j d_j\|),$$
for $\tilde{B}=$max$(|J(0)|,\hat{B}).$ Similarly,
$$|J(\theta_j-\delta_j d_j)|\leq \tilde{B}(1+\|\theta_j-\delta_j d_j\|).$$
From assumption A4 and triangle inequality, it follows that
$$\sup_j\Big{\|}\Big{(}J(\theta_j+\delta_j d_j)-J(\theta_j-\delta_j d_j)\Big{)}d_j\Big{\|}\leq \tilde{K}<\infty,$$
for some $\tilde{K}>0.$ Thus,
\begin{align*}
& \|\theta_{n+k}-\theta_n\| \leq \tilde{K}\sum_{j=n}^{n+k-1}\frac{a_j}{\delta_j}+\|\sum_{j=n}^{n+k-1}a_jM_{j+1}\|\\
& \rightarrow 0 \text{ a.s. with } n \rightarrow \infty.
\end{align*}
proving the lemma.
\end{proof}
\begin{lemma}
$\text{ For any } m \geq0,$\\
$$\Big{\|}\sum_{n=m}^{m+P-1}\frac{a_n}{a_m}D_n\nabla J(\theta_n)\Big{\|}\rightarrow 0 \text{ and }$$
$$\Big{\|}\sum_{n=m}^{m+P-1}\frac{b_n}{b_m}d_nJ(\theta_n)\Big{\|}\rightarrow 0,$$
$\text{almost surely, as } m \rightarrow \infty.$
\end{lemma}
\begin{proof}
 From lemma 3, it can be seen that 
 $\|\theta_{m+s}-\theta_{m}\|\rightarrow 0$ as $m\rightarrow \infty,$
 for all $s=1,\cdots,P.$ Also from assumption A1, we have
 $\|\nabla J(\theta_{m+s})-\nabla J(\theta_{m})\|\rightarrow 0$ as $m\rightarrow \infty,$
 for all $s=1,\cdots,P.$ Now from lemma 2 $\sum\limits_{n=m}^{m+P-1}D_n=0$ $\forall m\geq0.$
 Thus, 
 \begin{align*}
  &\Big{\|}\sum_{n=m}^{m+P-1}\frac{a_n}{a_m}D_n\nabla J(\theta_n)\Big{\|}\\
  & = \Big{\|}\sum_{n=m+1}^{m+P-1}\frac{a_n}{a_m}D_n\nabla J(\theta_n)
  +D_{m}\nabla J(\theta_{m})\Big{\|}\\
  &=\Big{\|}\sum_{n=m+1}^{m+P-1}\frac{a_n}{a_m}D_n \nabla J(\theta_n)
     -\sum_{n=m+1}^{m+P-1}D_n\nabla J(\theta_{m})\Big{\|}\\
  &=\Big{\|}\sum_{n=m+1}^{m+P-1}D_n\Big{(}\frac{a_n}{a_m}\nabla J(\theta_n)
  -\nabla J(\theta_{m})\Big{)}\Big{\|}\\
  &\leq \bar{K}\sum_{n=m+1}^{m+P-1}\Big{\|}\Big{(}\frac{a_n}{a_m}\nabla J(\theta_n)
  -\nabla J(\theta_{m})\Big{)}\Big{\|}\\
  &=\bar{K}\sum_{n=m+1}^{m+P-1}\Bigg{\|}\Big{(}\frac{a_n}{a_m}-1\Big{)}\nabla J(\theta_n)
   +\Big{(}\nabla J(\theta_n)-\nabla J(\theta_{m})\Big{)}\Bigg{\|}\\
  &\leq \bar{K}\sum_{n=m+1}^{m+P-1}\Bigg{\|}\Big{(}\frac{a_n}{a_m}-1\Big{)}\nabla J(\theta_n)\Bigg{\|}
  +\Bigg{\|}\nabla J(\theta_n)-\nabla J(\theta_{m})\Bigg{\|}\\
  &\text{ The claim now follows as a consequence of the}\\
  &\text{ assumptions A1 and A2.}\\
  &\text{ Now observe that $\|J(\theta_{m+k})-J(\theta_{m})\|\rightarrow 0$ as}\\
  &\text{$m\rightarrow \infty, $ for all $k \in \{1,\cdots,P\}.$ }\\
  &\text{ as a consequence of lemma3 and A1,}\\
  & \Bigg{\|}\sum_{n=m}^{m+P-1}\frac{b_n}{b_m}d_n J(\theta_n)\Bigg{\|}\\
  & =\Bigg{\|}\sum_{n=m+1}^{m+P-1}\frac{b_n}{b_m}d_n J(\theta_n)+d_m J(\theta_{m})\Bigg{\|}\\
  & =\Bigg{\|}\sum_{n=m+1}^{m+P-1}\frac{b_n}{b_m}d_n J(\theta_n)-\sum_{n=m+1}^{m+P-1}d_n J(\theta_{m})\Bigg{\|}\\
  & =\Bigg{\|}\sum_{n=m+1}^{m+P-1}d_n\Big{(}\frac{b_n}{b_m}J(\theta_n)-J(\theta_{m})\Big{)}\Bigg{\|}\\
  & \leq \sum_{n=m+1}^{m+P-1}\Big{\|}d_n\Big{(}\frac{b_n}{b_m}J(\theta_n)-J(\theta_{m})\Big{)}\Big{\|}\\
  & \leq \bar{\bar{K}}\sum_{n=m+1}^{m+P-1}\Big{\|}\Big{(}\frac{b_n}{b_m}J(\theta_n)-J(\theta_{m})\Big{)}\Big{\|}\\
  & =\bar{\bar{K}}\sum_{n=m+1}^{m+P-1} \Bigg{\|}\Big{(}\frac{b_n}{b_m}-1\Big{)} J(\theta_n)
  +\Big{(} J(\theta_n)-J(\theta_{m})\Big{)}\Bigg{\|}\\
  &\leq \bar{\bar{K}}\sum_{n=m+1}^{m+P-1}\Bigg{\|}\Big{(}\frac{b_n}{b_m}-1\Big{)} J(\theta_n)\Bigg{\|}
  +\Bigg{\|}\Big{(} J(\theta_n)- J(\theta_{m})\Big{)}\Bigg{\|}\\
\end{align*}
 The claim now follows as a consequence of the assumptions A1 and A2.
 \end{proof}
\begin{theorem}
 $\theta_n, n\geq0$ obtained from 1RDSA-2C satisfy $\theta_n \rightarrow G$
 almost surely.
\end{theorem}
\begin{proof}
 Note that
 \begin{align*}
  \begin{split}
  & \theta_{n+P} = \theta_n- \\ 
  &\sum_{l=n}^{n+P-1}a_l\Big{(}\frac{J(\theta_l+\delta_l d_l)-J(\theta_l-\delta_l d_l)}{2\delta_l}d_j+M_{l+1}\Big{)}
 \end{split}
 \end{align*}
It follows that
 \begin{align*}
  \begin{split}
  & \theta_{n+P} = \theta_n- \sum_{l=n}^{n+P-1}a_l\nabla J(\theta_l)\\ 
  & -\sum_{l=n}^{n+P-1}a_l(d_ld_l^T-I)\nabla J(\theta_l)\\
  & -\sum_{l=n}^{n+P-1}a_lo(\delta_l)
  -\sum_{l=n}^{n+P-1}a_lM_{l+1}
 \end{split}
 \end{align*}
Now the third term on the RHS can be written as
$$a_n\sum_{l=n}^{n+P-1}\frac{a_l}{a_n}D_{l}\nabla J(\theta_l)=a_n\xi_{n},$$
where $\xi_{n}=o(1)$ from lemma 4.
Thus, the algorithm is asymptotically analogous to
$$\theta_{n+1}=\theta_n-a_n(\nabla J(\theta_n)+o(\delta)+M_{n+1}).$$
Hence from chapter 2 of \cite{borkar2008stochastic} we have that $\theta_n$ converge to
local minima of the function $J.$
\end{proof}

\begin{theorem}
  $\theta_n, n\geq0$ obtained from 1RDSA-1C satisfy $\theta_n \rightarrow G$
 almost surely.
\end{theorem}
\begin{proof}
Note that
 \begin{align*}
 \theta_{n+P} = \theta_n-\sum_{l=n}^{n+P-1}a_l\Big{(}\frac{J(\theta_l+\delta_l d_l)}{2\delta_l}\Big{)}d_l-\sum_{l=n}^{n+P-1}a_lM_{l+1}
 \end{align*} 
It follows that
 \begin{align*}
  \begin{split}
  & \theta_{n+P} = \theta_n- \sum_{l=n}^{n+P-1}a_l\nabla J(\theta_l)\\ 
  & -\sum_{l=n}^{n+P-1}a_l\frac{J(\theta_l)}{\delta_l}d_l-\sum_{l=n}^{n+P-1}a_l(d_ld_l^T-I)\nabla J(\theta_l)\\
  & -\sum_{l=n}^{n+P-1}a_lO(\delta_l)
  -\sum_{l=n}^{n+P-1}a_lM_{l+1}
 \end{split}
 \end{align*}
 Now we observe that
 \begin{align*}
 & \sum_{l=n}^{n+P-1}a_l\frac{J(\theta_l)}{\delta_l}d_l= \sum_{l=n}^{n+P-1} b_{l}J(\theta_l)d_l\\
 & = b_n\sum_{l=n}^{n+P-1}\frac{b_l}{b_n}\frac{J(\theta_l)}{\delta_l}d_l=b_n\xi^{1}_{n},
 \end{align*}
 where $\xi^{1}_{n}=o(1)$ by lemma 4. Similarly
 $$\sum_{l=n}^{n+P-1}a_l(d_ld_l^T-I)\nabla J(\theta_l)=a_n\xi^{2}_{n},$$
 with $\xi^{2}_{n}=o(1)$ by lemma 4.
 The rest follows as explained in Theorem 5.
\end{proof}
